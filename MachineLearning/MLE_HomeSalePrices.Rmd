---
title: "Assessing the Impact of Outlier Removal and Feature Transformation in Multiple Linear Regression on Home Sale Prices"
author: "Wong, Courtney"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    toc_float: yes
    df_print: paged
    number_sections: true
---

```{r setup, echo=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	message = FALSE
)
```

```{r lib}
suppressPackageStartupMessages({
  library(tidyverse)
  library(knitr)
  library(Hmisc)
  library(caret)
  library(psych)
})
```

# Import and Format Data

First, we import the Home Sales dataset. This contains various home features and the target variable `SalesPrice`. The purpose of this exercise is to apply Multiple Linear Regression to predict a home's final sale price based on its features. After importing, we also manually correct some formatting issues with the file encoding, empty row at the bottom, meaningless row index feature, and comma in numerical cells.

```{r}
sales.df <- read.csv("https://s3.us-east-2.amazonaws.com/artificium.us/datasets/HomeSalesUFFIData.csv",
         header = TRUE,
         stringsAsFactors = FALSE, # all numeric
         fileEncoding="UTF-8-BOM", # removes BOM symbol from first header name
         nrows = 99) %>% # row 100 is empty
  mutate(SalesPrice = parse_number(SalesPrice)) %>% # get rid of comma to make numerical
  select(-Observation) # get rid of meaningless feature
```

# Identify Outliers

Next, we create a function to identify outliers in continuous features. We find a few outliers in `SalesPrice` and `LivingAreaSqFt`, remove them (a total of 4 observations), then save the version of the data with outliers removed in a different dataframe.

```{r}
# Create function to identify indices of Z-score outliers
index_outliers <- function(x) {
  x <- x[!is.na(x)]
  x_mean <- mean(x)
  x_sd <- sd(x)
  z_scores <- abs(x - x_mean) / x_sd
  # list("outliers" = x[z_scores > 2.8],
  #      "remaining" = x[z_scores <= 2.8],
  #      "indices" = z_scores > 2.8)
  return(z_scores > 2.8)
}
# Store indices of non-binary
numeric_var_ind <- apply(sales.df, MARGIN=2, function(x) length(unique(x)) > 2)
# List outliers for all numeric variables
lapply(sales.df[numeric_var_ind], function(x) {x[index_outliers(x)]})
# Create a mask that indicates whether any feature was an outlier for that row
mask <- lapply(sales.df[numeric_var_ind], function(x) {index_outliers(x)}) %>%
  bind_cols() %>%
  apply(MARGIN=1, any)
# Apply mask to create new df with outliers excluded
sales.no.df <- sales.df[!mask,]
# Visually check the rows that were excluded
sales.df[mask,]
```

# Check for Normal Distribution

We then apply the Shapiro-Wilk test in the outliers-removed dataset to check for normality. We also check for normality after transforming the data in various ways. Note that a p-value greater than 0.05 would indicate a normal distribution.

```{r}
# Visualize original distributions
hist.data.frame(sales.no.df[numeric_var_ind])
# Calculate Shapiro Wilk p value from original and various transformed distributions
original <- apply(sales.no.df[numeric_var_ind], MARGIN=2, function(x) shapiro.test(x)$p.value)
log10 <- apply(sales.no.df[numeric_var_ind], MARGIN=2, function(x) shapiro.test(log10(x+.001))$p.value)
inverse <- apply(sales.no.df[numeric_var_ind], MARGIN=2, function(x) shapiro.test(1/(x+.001))$p.value)
sqrt <- apply(sales.no.df[numeric_var_ind], MARGIN=2, function(x) shapiro.test(sqrt(x))$p.value)
rbind(original, log10, inverse, sqrt) %>% as.data.frame()
```

# Transform Non-Normally Distributed Data

Taking the optimal transform from above (whichever gives us the highest p-value), we transform the continuous variables. Note that we do not transform the target variable. Specifically, `LotSizeSqFt` is root-transformed, and `LivingAreaSqFt` is inverse-transformed. We then visualize the new distributions and confirm no NAs as a final check. While these may not be considered normal per the Shapiro-Wilk test, it should be noted that this does not invalidate the use of multiple linear regression.

```{r}
# Apply transforms
sales.tx <- sales.no.df %>%
  mutate(LotSizeSqFt = sqrt(LotSizeSqFt), # all values are positive integers
         LivingAreaSqFt = 1/LivingAreaSqFt) # all values are positive integers
# Re-check post-transform
hist.data.frame(sales.tx[numeric_var_ind])
# Confirm no NA
apply(sales.tx, MARGIN=2, function(x) sum(is.na(x)))
```

# Check For Colinearity

Next, we explore colinearity in the data.

```{r Q5_Colinearity}
pairs.panels(sales.df[numeric_var_ind])
pairs.panels(sales.no.df[numeric_var_ind])
pairs.panels(sales.tx[numeric_var_ind])
```

We notice the following notable correlations that are above 0.6 to the target variable:

-   LivingAreaSqFt in all 3 datasets
-   YearSold in the outlier-omitted datasets

There are no notable correlations between features, which suggests that each feature is non-redundant.

# Split Data for Training and Testing

Now, we can partition the data into 85% for training and 15% for testing. We use random sampling without replacement. Note that we choose to use the same randomly generated indices for the No Outliers dataset and the No Outliers + Data Transformed dataset, since they contain the same number of rows and this will allow for more direct comparison.

```{r Q6_SplitData}
set.seed(33452)

training_indices <- createDataPartition(sales.df$SalesPrice, times=1, p=0.85)[[1]]
sales.training <- sales.df[training_indices,]
sales.testing <- sales.df[-training_indices,]

training_indices <- createDataPartition(sales.no.df$SalesPrice, times=1, p=0.85)[[1]]
sales.no.training <- sales.no.df[training_indices,]
sales.no.testing <- sales.no.df[-training_indices,]

sales.tx.training <- sales.tx[training_indices,]
sales.tx.testing <- sales.tx[-training_indices,]
```

# Build Three Multiple Regression Models

Next, we build three multiple regression models from the three training datasets using a backward elimination based on p-value to predict `SalesPrice`. This means that the feature with the highest p-value is eliminated in each iteration until all features are significantly contributing. The three resulting models are shown below.

```{r Q7_Build3Models}
models <- map(list(sales.training, sales.no.training, sales.tx.training), function(x) {
  form_str <- "SalesPrice ~ ."
  m <- lm(as.formula(form_str), data=x)
  p_vals <- summary(m)$coefficients[,4]
  remaining <- names(x)[names(x) != "SalesPrice"] # initialize vector to store all feature names
  while(any(p_vals>0.05)) {
    elim <- which(p_vals == max(p_vals)) # obtain feature with highest p value
    remaining <- remaining[remaining != names(elim)] # remove the feature from the remaining list
    form_str <- paste("SalesPrice ~", paste(remaining, collapse=" + "))
    m <- lm(as.formula(form_str), data=x)
    p_vals <- summary(m)$coefficients[,4]
  }
  return(m)
})

lapply(models, summary)

```

Interestingly, we can see that all 3 models utilize 4 features, but the model built from the original dataset uses `HasPool` instead of `Finished.Bsment` like the other two.

# Assess the Three Multiple Regression Models

Next, we create a function to assess the Adjusted R-Squared and RMSE of each model on their respective testing datasets.

```{r Q8_AssessModels}
AssessModel <- function(m, testing_df, class_vec) {
  # testing_df: dataframe of testing cases, doesn't contain target variable
  # class_vec: vector of classifications for testing_df in the same order
  p <- predict(m, testing_df)
  results <- data.frame(Actual = class_vec,
                        Predicted = p) %>%
    mutate(SqErr = (Predicted - Actual)^2)
  return(list(AdjRsq = summary(m)$adj.r.squared,
              RMSE = sqrt(mean(results$SqErr))
              ))
}

testing_dfs <- list(select(sales.testing, -SalesPrice),
                    select(sales.no.testing, -SalesPrice),
                    select(sales.tx.testing, -SalesPrice))
class_vecs <- list(sales.testing$SalesPrice, sales.no.testing$SalesPrice, sales.tx.testing$SalesPrice)
data_names <- list("Original", "Outliers Removed", "Outliers Removed and Values Transformed")

# Map the function to each of the 3 model-testing-classification pairs
Map(AssessModel, models, testing_dfs, class_vecs) %>%
  bind_rows() %>%
  mutate(Dataset = data_names, .before="AdjRsq") %>%
  kable()
```

Based on the table above, the model created from the Original dataset has the best fit to its training set but has the greatest error when making predictions from the testing set. Thus, we can argue that the model created from the Outliers Removed dataset performed best in terms of error in its predictions: it has the next highest Adjusted R-squared and the lowest RMSE of all 3 models.

This poses an interesting question, however. Since outliers were removed from the latter 2 models, how well-equipped is it in handling unseen data in the future, which may contain outliers? Removing outliers, while it may produce the best performance metrics in predicting a dataset that is currently available, may not produce the best model to handle all future cases. Based on this, we could also argue that the model from the Original dataset is best, as it explains the most variation; it will inevitably have higher error when encountering outliers.

It should be noted that, of the `r nrow(sales.df[mask,])` outliers detected, `r nrow(inner_join(sales.training, sales.df[mask, ], by=colnames(sales.training)))` are present in the training set, and `r nrow(inner_join(sales.testing, sales.df[mask, ], by=colnames(sales.training)))` are in the testing set (by random splitting).

# 95% Prediction Intervals

Lastly, we calculate the 95% prediction interval for `SalesPrice` for each data point in the 3 validation datasets.

```{r}
# Calculate a df of actual, fit, lwr/upr prediction intervals for each dataset
p_intervals <- Map(function(m, testing_df, class_vec, data_name) {
  predict(m, testing_df, interval="prediction", level=0.95) %>%
    as.data.frame() %>%
    mutate(actual = class_vec,
           dataset = data_name, .before="actual")
}, models, testing_dfs, class_vecs, data_names)

# Check
p_intervals

# Visualize
map(p_intervals, ~ {
  data_name <- .x$dataset[1]
  .x %>%
    ggplot(aes(x=actual, y=fit)) +
    geom_point() +
    geom_errorbar(aes(
      ymin = lwr,
      ymax = upr
    )) +
    geom_abline(slope=1, intercept=0, color="red", linetype="dashed") +
    theme_bw() +
    labs(title = "Multiple Linear Regression Predictions from Validation Set",
         subtitle = data_name,
         x = "Actual Home Sale Price",
         y = "Predicted Home Sale Price",
         caption = "Red dashed line is the line of identity
         Vertical bars indicate 95% prediction intervals") +
    scale_x_continuous(labels = scales::dollar, limits=c(75000, 475000)) +
    scale_y_continuous(labels = scales::dollar, limits=c(75000, 675000))
})
  
  

```

The scatterplot for the "Original" dataset confirms our earlier observation: upon manual inspection, we can see that the point that deviates far from the line of identity is sale #40 in the original dataset, which was an outlier for `SalesPrice` ($`r sales.df[40,"SalesPrice"]`) *and* for `LivingAreaSqFt` (`r sales.df[40,"LivingAreaSqFt"]`). By chance, this outlier was selected for the validation subset and may explain the highest RMSE we saw in the previous section for the "Original" dataset. This datapoint was not present in the "Outliers Removed" and "Outliers Removed and Values Transformed" datasets, whose predictions had a lower RMSE. We can also observe that the prediction intervals are tighter for these two datasets compared to the "Original," understandably because there is less variation in the data with outliers removed.

# References

Becker, B. & Kohavi, R. (1996). Adult [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C5XW20.